
import time
import discord
import os
import re
from typing import Set
from transformers import GPT2TokenizerFast

import numpy as np
from dotenv import load_dotenv
from github import Github
import os
import jsonlines
import time
import pyparsing as pp
import openai
import pandas as pd
import tiktoken
from nltk.tokenize import sent_tokenize
from typing import List
from typing import Dict
from typing import Tuple

tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
COMPLETIONS_MODEL = "text-davinci-003"
EMBEDDING_MODEL = "text-embedding-ada-002"
MAX_SECTION_LEN = 500
SEPARATOR = "\n* "
ENCODING = "cl100k_base"

encoding = tiktoken.get_encoding(ENCODING)
separator_len = len(encoding.encode(SEPARATOR))

# f"Context separator contains {separator_len} tokens"


def find_between( s, first, last ):
    try:
        start = s.index( first ) + len( first )
        end = s.index( last, start )
        return s[start:end], end
    except ValueError:
        return "", 0

def find_all(s, ch):
    previous_ind = 0
    array = []
    length = len(s)
    while 1:
        try:
            ind = s.index(ch)
            array.append(ind + previous_ind)
            s = s[ind + len(ch):length]
            previous_ind = previous_ind + ind + len(ch)
        except ValueError:
            break
    return array

def remove_unwanted_char(s):
    code_separator = "```"
    index_array = find_all(s, code_separator)
    i = 0
    while i < len(index_array):
        start_index = index_array[i]
        i = i+1
        end_index = index_array[i]
        orig_string = s[start_index:end_index]
        replaced_string = orig_string.replace('#', '--')
        s = s.replace(orig_string, replaced_string)
        i = i+1
    return s

def get_needed_hash(s):
    s_array = s.split("\n")
    i = len(s_array) - 1
    req_no_of_hash = 2
    while i > 0:
        if s_array[i].find("#") != -1:
            req_no_of_hash = s_array[i].count('#') + 1
            print(s_array[i])
            break
        i = i - 1
    no_hash = 0
    hash_string = ''
    while no_hash < req_no_of_hash:
        hash_string = hash_string + '#'
        no_hash = no_hash + 1
    return hash_string

def cleanup_data(s):
    s = remove_unwanted_char(s)
    s = s.replace('<details>', '')
    s = s.replace('</details>', '')
    s = s.replace('</b></summary>', '')
    hash_string = get_needed_hash(s[0:s.find('<summary><b>')])
    s = s.replace('<summary><b>', hash_string)
    return s

def read_docs() -> []:
    g = Github(os.getenv('GITTOKEN'))
    repo = g.get_repo('abhisom2912/nft-drop-starter-project')
    title_stack=[]
    contents = repo.get_contents("")
    while contents:
        try:
            file_content = contents.pop(0)
        except Exception:
            pass
        if file_content.type == "dir" and file_content.path.find( 'validators') != -1:
            print(file_content.name)
            contents.extend(repo.get_contents(file_content.path))
        else:
            if file_content.name == 'sentry-node-devnet.md':
                print('found it')
                file_contents = repo.get_contents(file_content.path)
                title = pp.AtLineStart(pp.Word("#")) + pp.rest_of_line
                sample = file_contents.decoded_content.decode()
                sample = cleanup_data(sample)

                title_stack.append([0, 'start_of_file'])
                if(sample.split('\n')[0] == '---'):
                    title_stack[-1].append('')
                    title_stack[-1].append(file_content.path)
                    title_stack.append([1, sample.split('\n')[1].split(':')[1].lstrip()])
                    sample = sample.split('---')[2]

                last_end = 0
                for t, start, end in title.scan_string(sample):
                    # save content since last title in the last item in title_stack
                    title_stack[-1].append(sample[last_end:start].lstrip("\n"))
                    title_stack[-1].append(file_content.path)

                    # add a new entry to title_stack
                    marker, title_content = t
                    level = len(marker)
                    title_stack.append([level, title_content.lstrip()])

                    # update last_end to the end of the current match
                    last_end = end

                # add trailing text to the final parsed title
                title_stack[-1].append(sample[last_end:])
                title_stack[-1].append(file_content.path)
    return title_stack


	
def main():
    outputs = read_docs()
    print(outputs)
    # s = "acb &&&&```cat```&&&& $$ @@dog$^"
    # result = re.findall(r"&&&&.*?&&&&", s)
    # print(result)

if __name__ == '__main__':
    main()